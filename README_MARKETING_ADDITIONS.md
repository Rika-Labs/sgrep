# README Marketing Content - Performance Benchmarks

This document contains the proposed marketing content for the README once we achieve the 50K files in <60s target.

---

## ‚ö° Performance Benchmarks Section

Add this section after "Quick Start" and before "Coding Agent Integration":

```markdown
## ‚ö° Blazing Fast Performance

**Guaranteed Throughput:** 833+ files/second
**Verified on:** Ubuntu 22.04, 8-core Intel i7, 16GB RAM

sgrep is engineered for speed. Every commit is benchmarked to ensure we meet strict performance SLAs.

| Repo Size | Index Time | Throughput | Search P95 | Verified |
|-----------|------------|------------|-----------|----------|
| 1K files | <3s | 333+ files/s | <30 ms | ‚úÖ [actix-web](benches/repos) |
| 10K files | <15s | 666+ files/s | <80 ms | ‚úÖ [synthetic](benches/repos) |
| **50K files** | **<60s** | **833+ files/s** | <200 ms | ‚úÖ [synthetic](benches/repos) |
| 100K files | <2m | 833+ files/s | <300 ms | ‚úÖ [synthetic](benches/repos) |

> üí° **Performance scales with CPU cores.** Results from 8-core system; 16-core systems see proportionally faster indexing.

### How We Achieved This

- **üîÄ Work-Stealing Parallelism:** Dynamic load balancing across embedder pool prevents idle cores
- **üì¶ Aggressive Batching:** Process 2048 chunks/batch on GPU, 512 on CPU with smart token budgeting
- **üíæ Persistent Caching:** Cross-repo deduplication saves 20-40% on common code patterns
- **üéØ Smart Chunking:** Skip expensive tree-sitter parsing for simple files (JSON, YAML, configs)
- **‚ö° SIMD Everywhere:** AVX-512/AVX2 accelerated vector operations via SimSIMD
- **üóÑÔ∏è Zero-Copy I/O:** Memory-mapped index loading for instant search startup

See [PERFORMANCE_OPTIMIZATION_PLAN.md](PERFORMANCE_OPTIMIZATION_PLAN.md) for technical deep dive.

### Run the Benchmarks Yourself

```bash
# Clone sgrep
git clone https://github.com/rika-labs/sgrep
cd sgrep

# Generate test repositories (requires Python)
cd benches/repos && ./download_repos.sh && cd ../..

# Run full benchmark suite
cargo bench --bench bench_indexing

# Quick SLA check: verify 50K files in <60s
cargo test --release --test performance_sla sla_50k_files_under_60s -- --ignored --nocapture
```

All benchmarks run in CI on every commit: [View Latest Results](https://github.com/rika-labs/sgrep/actions/workflows/benchmark.yml)
```

---

## Badge Updates

Add these badges to the top of the README:

```markdown
[![Performance: 833+ files/s](https://img.shields.io/badge/Performance-833%2B%20files%2Fs-brightgreen)](benches/)
[![Benchmarks](https://github.com/rika-labs/sgrep/actions/workflows/benchmark.yml/badge.svg)](https://github.com/rika-labs/sgrep/actions/workflows/benchmark.yml)
```

---

## Social Media Snippets

### Twitter/X Post
```
üöÄ sgrep just got FAST.

We re-engineered our indexing pipeline to guarantee 833+ files/second:
- 50K file codebases in <60s
- Work-stealing parallelism
- Persistent embedding cache
- SIMD-accelerated vectors

Local-first semantic search that actually scales.

[link to repo]
```

### LinkedIn Post
```
After weeks of optimization, I'm excited to share that sgrep now indexes 50,000-file codebases in under 60 seconds.

Key innovations:
‚úÖ Work-stealing parallelism across embedder pool
‚úÖ Aggressive batching with smart token budgeting
‚úÖ Persistent cross-repo embedding cache
‚úÖ SIMD-accelerated vector operations

Every optimization is benchmarked and tested in CI. Performance isn't a feature‚Äîit's a guarantee.

Full technical breakdown: [link to PERFORMANCE_OPTIMIZATION_PLAN.md]

#rust #developer-tools #performance-engineering
```

### Hacker News Post Title
```
Sgrep: Local semantic code search that indexes 50K files in <60s
```

### Hacker News Comment (First Comment)
```
Author here. After profiling sgrep's indexing pipeline, we found that embedding generation consumed 60-80% of total time.

We implemented several optimizations:
- Work-stealing queue for embedder pool (replaced round-robin)
- Increased batch sizes (256‚Üí512 CPU, 512‚Üí2048 GPU)
- Persistent embedding cache with BLAKE3 deduplication
- Fast-path chunking that skips tree-sitter for simple files
- GPU batch pipelining (overlap CPU prep with GPU inference)

Result: 2.5x speedup, guaranteed 833+ files/sec throughput.

All benchmarks run in CI and verified with real/synthetic repos. Full technical plan: [link]

Happy to answer questions!
```

### Reddit r/rust Post
```
[Project] sgrep: Semantic code search optimized for 50K+ file codebases

I've been working on sgrep, a local-first semantic code search tool powered by BGE embeddings. After extensive profiling and optimization, it now indexes 50K files in under 60 seconds (833+ files/s).

Key tech:
- Rust + rayon for parallel chunking/embedding
- PooledEmbedder with work-stealing queue
- Persistent BLAKE3-keyed embedding cache
- ONNX runtime with SimSIMD for vector ops
- Comprehensive criterion benchmarks in CI

The optimization journey was fascinating‚Äîembedding inference was 60-80% of time, so we focused there with larger batches, better load balancing, and GPU pipelining.

Check it out: [link]
Full optimization plan: [link to PERFORMANCE_OPTIMIZATION_PLAN.md]
```

---

## Blog Post Outline

**Title:** "How We Made sgrep Index 50,000 Files in Under a Minute"

**Sections:**

1. **Introduction**
   - What is sgrep? (local semantic code search)
   - The challenge: Fast indexing for large codebases
   - The goal: 50K files in <60s (833+ files/s)

2. **Profiling & Bottleneck Identification**
   - Initial performance: ~333 files/s
   - Profiling revealed: 60-80% time in embedding
   - The plan: Focus on embedding pipeline

3. **Optimization 1: Work-Stealing Embedder Pool**
   - Problem: Round-robin caused idle cores with uneven batches
   - Solution: crossbeam work-stealing queue
   - Result: 10-20% faster embedding

4. **Optimization 2: Aggressive Batching**
   - Problem: Conservative batch sizes (256 CPU, 512 GPU)
   - Solution: Increase to 512 CPU, 2048 GPU with RAM checks
   - Result: 1.5x faster on GPU, 1.3x on CPU

5. **Optimization 3: Persistent Embedding Cache**
   - Problem: Common code patterns re-embedded every run
   - Solution: Disk-backed cache keyed by BLAKE3 hash
   - Result: 20-40% speedup on repos with boilerplate

6. **Optimization 4: Smart Chunking**
   - Problem: Tree-sitter overhead on simple files
   - Solution: Fast-path for JSON/YAML/Markdown
   - Result: 5-10% reduction in chunking time

7. **Optimization 5: GPU Batch Pipelining**
   - Problem: Sequential batch processing
   - Solution: Async pipeline (prep batch N+1 while GPU runs N)
   - Result: 15-25% faster on GPU

8. **Results**
   - Before/after comparison table
   - Benchmark suite design
   - CI integration for regression prevention

9. **Lessons Learned**
   - Profile first, optimize later
   - Focus on the bottleneck (80/20 rule)
   - Verifiable benchmarks > anecdotal claims
   - Performance is a feature, treat it like one

10. **What's Next**
    - 100K files in <2min
    - Distributed indexing
    - More embedding models

---

## Documentation Updates

### Performance Tuning Guide

Create `docs/PERFORMANCE_TUNING.md`:

```markdown
# Performance Tuning Guide

sgrep is fast by default, but you can tune it for your specific hardware.

## Environment Variables

### Embedder Pool Size
`SGREP_EMBEDDER_POOL_SIZE=N`

Controls how many embedding model instances run in parallel.

- **Default:** `min(CPU_cores, 8)`
- **Recommended:**
  - Low RAM (<8GB): `SGREP_EMBEDDER_POOL_SIZE=2`
  - Normal (8-16GB): Use default
  - High RAM (>16GB): `SGREP_EMBEDDER_POOL_SIZE=16`

### Batch Size
`SGREP_BATCH_SIZE=N`

Number of chunks to embed per batch.

- **Default:** 256 (CPU), 512 (GPU)
- **Recommended:**
  - CPU-only: `SGREP_BATCH_SIZE=512`
  - GPU (CUDA/CoreML): `SGREP_BATCH_SIZE=2048`
  - Low RAM: `SGREP_BATCH_SIZE=128`

### Hardware Acceleration
`SGREP_DEVICE=cpu|cuda|coreml`

Force specific hardware backend.

- **Default:** Auto-detect (CoreML on Apple Silicon, CUDA on NVIDIA, CPU otherwise)
- **Override:** Set explicitly if auto-detect fails

### Rayon Thread Count
`RAYON_NUM_THREADS=N`

Limit parallel threads for chunking/batching.

- **Default:** All CPU cores
- **Recommended:**
  - Laptop (thermal constraints): `RAYON_NUM_THREADS=4`
  - Server: Use default

## Optimization Checklist

### For Maximum Speed

```bash
# Use GPU if available
export SGREP_DEVICE=cuda  # or coreml on Mac

# Increase batch size
export SGREP_BATCH_SIZE=2048

# Increase pool size (if RAM permits)
export SGREP_EMBEDDER_POOL_SIZE=16

sgrep index
```

### For Low Memory Systems

```bash
# Reduce pool size
export SGREP_EMBEDDER_POOL_SIZE=2

# Reduce batch size
export SGREP_BATCH_SIZE=128

# Limit rayon threads
export RAYON_NUM_THREADS=2

sgrep index
```

### For Laptops (Balance Speed/Heat)

```bash
# Moderate pool size
export SGREP_EMBEDDER_POOL_SIZE=4

# Moderate batch size
export SGREP_BATCH_SIZE=512

# Limit CPU usage
export RAYON_NUM_THREADS=4

sgrep index
```

## Troubleshooting

### "Out of memory" errors
- Reduce `SGREP_EMBEDDER_POOL_SIZE`
- Reduce `SGREP_BATCH_SIZE`
- Close other applications

### Slow indexing on GPU
- Verify GPU is detected: `RUST_LOG=sgrep=debug sgrep index`
- Try different batch sizes: `SGREP_BATCH_SIZE=1024` or `2048`
- Check CUDA/CoreML drivers

### High CPU but slow progress
- Bottleneck might be disk I/O, not CPU
- Use SSD for index location (`SGREP_HOME`)
- Check if antivirus is scanning files

### Thermal throttling on laptop
- Reduce `RAYON_NUM_THREADS=2`
- Enable `--profile` to see time breakdown
- Index during cooler ambient temps
```

---

**Last Updated:** 2025-11-24
**Status:** Ready for publication after optimizations are implemented
